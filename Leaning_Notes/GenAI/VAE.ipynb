{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程来源 [抛开数学，轻松学懂 VAE](https://zhouyifan.net/2022/12/19/20221016-VAE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###下载好了图片后，可以用下面的代码创建Dataloader\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root, img_shape=(64, 64)) -> None:\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.img_shape = img_shape\n",
    "        self.filenames = sorted(os.listdir(root))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        path = os.path.join(self.root, self.filenames[index])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        pipeline = transforms.Compose([\n",
    "            transforms.CenterCrop(168),\n",
    "            transforms.Resize(self.img_shape),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        return pipeline(img)\n",
    "\n",
    "\n",
    "def get_dataloader(root='data/celebA/img_align_celeba', **kwargs):\n",
    "    dataset = CelebADataset(root, **kwargs)\n",
    "    return DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_path = 'data/celebA/img_align_celeba'\n",
    "    dataloader = get_dataloader()\n",
    "    img = next(iter(dataloader))\n",
    "    print(img.shape)\n",
    "    # Concat 4x4 images\n",
    "    N, C, H, W = img.shape\n",
    "    assert N == 16\n",
    "    img = torch.permute(img, (1, 0, 2, 3))\n",
    "    img = torch.reshape(img, (C, 4, 4 * H, W))\n",
    "    img = torch.permute(img, (0, 2, 1, 3))\n",
    "    img = torch.reshape(img, (C, 4 * H, 4 * W))\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img.save('work_dirs/tmp.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VAE maps inputs into a multivariate normal distribution](https://hackernoon.com/how-to-sample-from-latent-space-with-variational-autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    VAE for 64x64 face generation. The hidden dimensions can be tuned.\n",
    "    '''\n",
    "    def __init__(self, hiddens=[16, 32, 64, 128, 256], latent_dim=128) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        prev_channels = 3\n",
    "        modules = []\n",
    "        img_length = 64\n",
    "        for cur_channels in hiddens:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(prev_channels,\n",
    "                              cur_channels,\n",
    "                              kernel_size=3,\n",
    "                              stride=2,\n",
    "                              padding=1), nn.BatchNorm2d(cur_channels),\n",
    "                    nn.ReLU()))\n",
    "            prev_channels = cur_channels\n",
    "            img_length //= 2\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.mean_linear = nn.Linear(prev_channels * img_length * img_length,\n",
    "                                     latent_dim)\n",
    "        self.var_linear = nn.Linear(prev_channels * img_length * img_length,\n",
    "                                    latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        # decoder\n",
    "        modules = []\n",
    "        self.decoder_projection = nn.Linear(\n",
    "            latent_dim, prev_channels * img_length * img_length)\n",
    "        self.decoder_input_chw = (prev_channels, img_length, img_length)\n",
    "        for i in range(len(hiddens) - 1, 0, -1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hiddens[i],\n",
    "                                       hiddens[i - 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride=2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hiddens[i - 1]), nn.ReLU()))\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(hiddens[0],\n",
    "                                   hiddens[0],\n",
    "                                   kernel_size=3,\n",
    "                                   stride=2,\n",
    "                                   padding=1,\n",
    "                                   output_padding=1),\n",
    "                nn.BatchNorm2d(hiddens[0]), nn.ReLU(),\n",
    "                nn.Conv2d(hiddens[0], 3, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU()))\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        print('encoded.shape',encoded.shape)##torch.Size([16, 256, 2, 2])\n",
    "        encoded = torch.flatten(encoded, 1)\n",
    "        mean = self.mean_linear(encoded)\n",
    "        print('mean.shape',mean.shape)##torch.Size([16, latent_dim = 128])\n",
    "        logvar = self.var_linear(encoded)\n",
    "        print('logvar.shape',logvar.shape)##torch.Size([16, latent_dim = 128])\n",
    "        eps = torch.randn_like(logvar)\n",
    "        std = torch.exp(logvar / 2)\n",
    "        z = eps * std + mean\n",
    "        x = self.decoder_projection(z)\n",
    "        x = torch.reshape(x, (-1, *self.decoder_input_chw))\n",
    "        decoded = self.decoder(x)\n",
    "\n",
    "        return decoded, mean, logvar\n",
    "    def sample(self, device='cuda'):\n",
    "        z = torch.randn(1, self.latent_dim).to(device)\n",
    "        x = self.decoder_projection(z)\n",
    "        x = torch.reshape(x, (-1, *self.decoder_input_chw))\n",
    "        decoded = self.decoder(x)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 来自chatgpt\n",
    "\n",
    "### KL散度计算公式\n",
    "\n",
    "假设 \\( z \\) 是潜在变量，其后验分布 \\( q(z|x) \\) 为正态分布 \\( N(\\mu, \\sigma^2) \\)，其中 \\( \\mu \\) 和 \\( \\sigma \\) 是由神经网络从数据 \\( x \\) 中学到的。如果先验分布 \\( p(z) \\) 是标准正态分布 \\( N(0,1) \\)，那么KL散度的计算公式为：\n",
    "$$\\text{KL}(q(z|x) || p(z)) = -\\frac{1}{2} \\sum_{i=1}^d \\left( 1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2 \\right)$$\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "```python\n",
    "kl_loss = torch.mean(\n",
    "    -0.5 * torch.sum(1 + logvar - mean**2 - torch.exp(logvar), 1), 0)\n",
    "```\n",
    "\n",
    "- `mean` 和 `logvar` 分别代表 \\( \\mu \\) 和 \\( \\log(\\sigma^2) \\)。\n",
    "- `torch.sum(..., 1)` 对每个样本的所有维度求和，计算出每个样本的KL散度。\n",
    "- `torch.mean(..., 0)` 计算所有样本的平均KL散度，这是整个数据批次的平均KL散度。\n",
    "\n",
    "以上代码段在计算每个维度的KL散度后，对所有维度求和，并最后计算所有样本的平均值，从而得到整个数据批次的平均KL散度。\n",
    "----------------------------------------\n",
    "要完整地展示变分自编码器中KL散度公式从积分形式到简化表达式的代数简化过程，我们将详细解析每一步。这涉及到一些复杂的数学操作，包括完成平方和积分。下面是详细的推导步骤：\n",
    "\n",
    "### 初始积分形式\n",
    "我们从KL散度的定义开始：\n",
    "$$\n",
    "\\text{KL}(q(z|x) || p(z)) = \\int q(z|x) \\log \\frac{q(z|x)}{p(z)} dz\n",
    "$$\n",
    "代入正态分布的密度函数，\\( q(z|x) = N(\\mu, \\sigma^2) \\) 和 \\( p(z) = N(0, 1) \\)，我们有：\n",
    "$$\n",
    "\\log \\frac{q(z|x)}{p(z)} = \\log \\left(\\frac{1}{\\sigma} e^{-\\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2}} \\right)\n",
    "$$\n",
    "$$\n",
    "= -\\log \\sigma - \\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2}\n",
    "$$\n",
    "\n",
    "### 展开和简化\n",
    "现在我们要将这些项进一步展开和简化：\n",
    "$$\n",
    "-\\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2} = -\\frac{z^2 - 2z\\mu + \\mu^2}{2\\sigma^2} + \\frac{z^2}{2}\n",
    "$$\n",
    "我们可以重写这个表达式为：\n",
    "$$\n",
    "-\\frac{z^2}{2\\sigma^2} + \\frac{z\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} + \\frac{z^2}{2}\n",
    "$$\n",
    "将相同项 \\(z^2\\) 合并：\n",
    "$$\n",
    "\\left(\\frac{1}{2} - \\frac{1}{2\\sigma^2}\\right) z^2 + \\frac{z\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}\n",
    "$$\n",
    "简化：\n",
    "$$\n",
    "\\frac{1-\\sigma^2}{2\\sigma^2} z^2 + \\frac{z\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "### 计算积分\n",
    "现在，将这个结果代入积分中。利用高斯积分的性质，只有 \\(z\\) 的平方项和常数项会对积分产生非零贡献，而线性项的积分为零（因为高斯函数关于其均值对称）：\n",
    "$$\\text{KL}(q(z|x) || p(z)) = -\\log \\sigma - \\int \\left(\\frac{1-\\sigma^2}{2\\sigma^2} z^2 + \\frac{\\mu^2}{2\\sigma^2}\\right) q(z|x) dz\n",
    "$$\n",
    "利用高斯分布 \\(q(z|x)\\) 的期望性质，我们有：\n",
    "$$\n",
    "\\int z^2 q(z|x) dz = \\sigma^2 + \\mu^2\n",
    "$$\n",
    "代入上述公式，我们可以得到：\n",
    "$$\n",
    "\\text{KL}(q(z|x) || p(z)) = -\\log \\sigma - \\left(\\frac{1-\\sigma^2}{2\\sigma^2} (\\sigma^2 + \\mu^2) + \\frac{\\mu^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "这进一步简化为：\n",
    "$$\n",
    "= -\\log \\sigma - \\frac{1-\\sigma^2}{2\\sigma^2} \\sigma^2 - \\frac{\\mu^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}\n",
    "$$\n",
    "$$\n",
    "= -\\log \\sigma - \\frac{1-\\sigma^2}{2} - \\frac{\\mu^2}{\\sigma^2}\n",
    "$$\n",
    "最后，化简得到：\n",
    "$$\n",
    "\\text{KL}(q(z|x) || p(z)) = \\frac{1}{2} (\\mu^2 + \\sigma^2 - \\log \\sigma^2 - 1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# from dldemos.VAE.load_celebA import get_dataloader\n",
    "# from dldemos.VAE.model import VAE\n",
    "\n",
    "# Hyperparameters\n",
    "n_epochs = 1\n",
    "# n_epochs = 10\n",
    "kl_weight = 0.00025\n",
    "lr = 0.005\n",
    "\n",
    "\n",
    "def loss_fn(y, y_hat, mean, logvar):\n",
    "    recons_loss = F.mse_loss(y_hat, y)\n",
    "    kl_loss = torch.mean(\n",
    "        -0.5 * torch.sum(1 + logvar - mean**2 - torch.exp(logvar), 1), 0)\n",
    "    loss = recons_loss + kl_loss * kl_weight\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, dataloader, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    dataset_len = len(dataloader.dataset)\n",
    "\n",
    "    begin_time = time()\n",
    "    # train\n",
    "    for i in range(n_epochs):\n",
    "        loss_sum = 0\n",
    "        for x in dataloader:\n",
    "            # print('x.shape',x.shape)\n",
    "            x = x.to(device)\n",
    "            y_hat, mean, logvar = model(x)\n",
    "            loss = loss_fn(x, y_hat, mean, logvar)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss\n",
    "            break##只训练一次 为了看看输出\n",
    "        loss_sum /= dataset_len\n",
    "        training_time = time() - begin_time\n",
    "        minute = int(training_time // 60)\n",
    "        second = int(training_time % 60)\n",
    "        print(f'epoch {i}: loss {loss_sum} {minute}:{second}')\n",
    "        torch.save(model.state_dict(), 'dldemos/VAE/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape torch.Size([16, 256, 2, 2])\n",
      "mean.shape torch.Size([16, 128])\n",
      "logvar.shape torch.Size([16, 128])\n",
      "epoch 0: loss 2.5405049655091716e-06 0:0\n"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataloader = get_dataloader()\n",
    "model.to(device)\n",
    "train(device, dataloader, model)\n",
    "# model.load_state_dict(torch.load('dldemos/VAE/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##定义生成图片的函数\n",
    "def generate(device, model):\n",
    "    model.eval()\n",
    "    output = model.sample(device)\n",
    "    output = output[0].detach().cpu()\n",
    "    img = ToPILImage()(output)\n",
    "    img.save('work_dirs/tmp.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###生成图片\n",
    "generate(device, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
