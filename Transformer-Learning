针对 ECG 构建深度学习模型时需要补充的基础知识
# Transformer 学习资料
- 李沐动手深度学习的英文版第11章都是讲Transformer的，可以从头看，从attention机制看起，也可以直接看[11.7](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html)，这个章节讲了Transformer的结构
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) 这个博客对Transformer做了很多可视化，非常便于理解
- [Illustrated Guide to Transformers- Step by Step Explanation](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)
- [Self-Attention和Transformer](https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer)这篇中文博客融合了其他很多博客的内容
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) 针对 Transformer 原始论文《Attention is all you need》用代码来详细介绍算法，并给出训练和推理过程的解释
- [A walkthrough of transformer architecture code](https://github.com/markriedl/transformer-walkthrough/tree/main) 一张详细的Transformer图，包括变量维度等详细信息
- [The Transformer Family Version 2.0](https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500)
## Transformer 子模块的解释相关资料
- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition)
- [详解softmax函数以及相关求导过程](https://zhuanlan.zhihu.com/p/25723112)
- [Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients](https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500)
## 相关学习资料
- [From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf):关于flash attention 的博客
# Transformer for Vision
- Dive into deep learning [11.8 Transformers for Vision][https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html]
- 一篇介绍 Transformer 在计算机视觉的应用(Transformers in Vision)[https://iaml-it.github.io/posts/2021-04-28-transformers-in-vision/]



